{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5711786e-64f7-4b2c-ac5a-9a197531fd20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4782ed08-1d9a-45bf-9393-bb75a8d7f99d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check imports\n",
    "try:\n",
    "  import torch\n",
    "  import torch.nn as nn\n",
    "  import pytorch_lightning as pl\n",
    "  from torch.utils.data import DataLoader, random_split, Dataset\n",
    "  import matplotlib.pyplot as plt\n",
    "  import wandb\n",
    "  from pytorch_lightning.loggers import WandbLogger\n",
    "  from pytorch_lightning.callbacks import Callback\n",
    "  from pytorch_msssim import ssim\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"Exception = {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import yaml\n",
    "\n",
    "import utils\n",
    "from multimae.criterion import MaskedMSELoss\n",
    "from multimae.input_adapters import PatchedInputAdapter\n",
    "from multimae.output_adapters import SpatialOutputAdapter\n",
    "from utils import NativeScalerWithGradNormCount as NativeScaler\n",
    "from utils import create_model\n",
    "from utils.datasets_chloe import build_multimae_pretraining_dataset\n",
    "from utils.optim_factory import create_optimizer\n",
    "from utils.task_balancing import (NoWeightingStrategy,\n",
    "                                  UncertaintyWeightingStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('MultiMAE pre-training script', add_help=False)\n",
    "parser.add_argument('--batch_size', default=32, type=int,\n",
    "                    help='Batch size per GPU (default: %(default)s)')\n",
    "parser.add_argument('--epochs', default=1600, type=int,\n",
    "                    help='Number of epochs (default: %(default)s)')\n",
    "parser.add_argument('--save_ckpt_freq', default=1, type=int,\n",
    "                    help='Checkpoint saving frequency in epochs (default: %(default)s)')\n",
    "# Task parameters\n",
    "parser.add_argument('--in_domains', default='s1-s2', type=str,\n",
    "                    help='Input domain names, separated by hyphen (default: %(default)s)')\n",
    "parser.add_argument('--out_domains', default='s1-s2', type=str,\n",
    "                    help='Output domain names, separated by hyphen (default: %(default)s)')\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument('--model', default='pretrain_multimae_base', type=str, metavar='MODEL',\n",
    "                    help='Name of model to train (default: %(default)s)')\n",
    "parser.add_argument('--num_encoded_tokens', default=1568, type=int,\n",
    "                    help='Number of tokens to randomly choose for encoder (default: %(default)s)')\n",
    "parser.add_argument('--num_global_tokens', default=1, type=int,\n",
    "                    help='Number of global tokens to add to encoder (default: %(default)s)')\n",
    "parser.add_argument('--patch_size', default=4, type=int,\n",
    "                    help='Base patch size for image-like modalities (default: %(default)s)')\n",
    "parser.add_argument('--input_size', default=224, type=int,\n",
    "                    help='Images input size for backbone (default: %(default)s)')\n",
    "parser.add_argument('--alphas', type=float, default=0.3, \n",
    "                    help='Dirichlet alphas concentration parameter (default: %(default)s)')\n",
    "parser.add_argument('--sample_tasks_uniformly', default=True, action='store_true',\n",
    "                    help='Set to True/False to enable/disable uniform sampling over tasks to sample masks for.')\n",
    "parser.add_argument('--decoder_use_task_queries', default=True, action='store_true',\n",
    "                    help='Set to True/False to enable/disable adding of task-specific tokens to decoder query tokens')\n",
    "parser.add_argument('--decoder_use_xattn', default=True, action='store_true',\n",
    "                    help='Set to True/False to enable/disable decoder cross attention.')\n",
    "parser.add_argument('--decoder_dim', default=256, type=int,\n",
    "                    help='Token dimension inside the decoder layers (default: %(default)s)')\n",
    "parser.add_argument('--decoder_depth', default=2, type=int,\n",
    "                    help='Number of self-attention layers after the initial cross attention (default: %(default)s)')\n",
    "parser.add_argument('--decoder_num_heads', default=8, type=int,\n",
    "                    help='Number of attention heads in decoder (default: %(default)s)')\n",
    "parser.add_argument('--drop_path', type=float, default=0.0, metavar='PCT',\n",
    "                    help='Drop path rate (default: %(default)s)')\n",
    "parser.add_argument('--loss_on_unmasked', default=False, action='store_true',\n",
    "                    help='Set to True/False to enable/disable computing the loss on non-masked tokens')\n",
    "parser.add_argument('--no_loss_on_unmasked', action='store_false', dest='loss_on_unmasked')\n",
    "parser.set_defaults(loss_on_unmasked=False)\n",
    "# Optimizer parameters\n",
    "parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                    help='Optimizer (default: %(default)s)')\n",
    "parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                    help='Optimizer epsilon (default: %(default)s)')\n",
    "parser.add_argument('--opt_betas', default=[0.9, 0.95], type=float, nargs='+', metavar='BETA',\n",
    "                    help='Optimizer betas (default: %(default)s)')\n",
    "parser.add_argument('--clip_grad', type=float, default=None, metavar='CLIPNORM',\n",
    "                    help='Clip gradient norm (default: %(default)s)')\n",
    "parser.add_argument('--skip_grad', type=float, default=None, metavar='SKIPNORM',\n",
    "                    help='Skip update if gradient norm larger than threshold (default: %(default)s)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: %(default)s)')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                    help='Weight decay (default: %(default)s)')\n",
    "parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n",
    "    weight decay. We use a cosine schedule for WD.  (Set the same value as args.weight_decay to keep weight decay unchanged)\"\"\")\n",
    "parser.add_argument('--decoder_decay', type=float, default=None, help='decoder weight decay')\n",
    "parser.add_argument('--blr', type=float, default=1e-4, metavar='LR',\n",
    "                    help='Base learning rate: absolute_lr = base_lr * total_batch_size / 256 (default: %(default)s)')\n",
    "parser.add_argument('--warmup_lr', type=float, default=1e-6, metavar='LR',\n",
    "                    help='Warmup learning rate (default: %(default)s)')\n",
    "parser.add_argument('--min_lr', type=float, default=0., metavar='LR',\n",
    "                    help='Lower lr bound for cyclic schedulers that hit 0 (default: %(default)s)')\n",
    "parser.add_argument('--task_balancer', type=str, default='none',\n",
    "                    help='Task balancing scheme. One out of [uncertainty, none] (default: %(default)s)')\n",
    "parser.add_argument('--balancer_lr_scale', type=float, default=1.0,\n",
    "                    help='Task loss balancer LR scale (if used) (default: %(default)s)')\n",
    "parser.add_argument('--warmup_epochs', type=int, default=0, metavar='N',\n",
    "                    help='Epochs to warmup LR, if scheduler supports (default: %(default)s)')\n",
    "parser.add_argument('--warmup_steps', type=int, default=-0, metavar='N',\n",
    "                    help='Epochs to warmup LR, if scheduler supports (default: %(default)s)')\n",
    "parser.add_argument('--fp32_output_adapters', type=str, default='',\n",
    "                    help='Tasks output adapters to compute in fp32 mode, separated by hyphen.')\n",
    "# Augmentation parameters\n",
    "parser.add_argument('--hflip', type=float, default=0.5,\n",
    "                    help='Probability of horizontal flip (default: %(default)s)')\n",
    "parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                    help='Training interpolation (random, bilinear, bicubic) (default: %(default)s)')\n",
    "# Dataset parameters\n",
    "parser.add_argument('--data_path', type=str, default=None,\n",
    "                    help='(optional) base dir if your txt paths are relative.')\n",
    "parser.add_argument(\n",
    "    '--s1_txt',\n",
    "    type=str,\n",
    "    default=\"/scratch/bepk/bkim2/MultiMAE_RGB/MultiMAE/valid_list/delta/30m/pair_S1.txt\",\n",
    "    help=\"Path to modis txt file\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--s2_txt',\n",
    "    type=str,\n",
    "    default=\"/scratch/bepk/bkim2/MultiMAE_RGB/MultiMAE/valid_list/delta/30m/pair_S2.txt\",\n",
    "    help=\"Path to s2 txt file\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--all_domains',\n",
    "    type=str,\n",
    "    default='s1-s2',\n",
    "    help='All domain names, separated by hyphen'\n",
    ")\n",
    "\n",
    "\n",
    "parser.add_argument('--imagenet_default_mean_and_std', default=False, action='store_true')\n",
    "# Misc.\n",
    "parser.add_argument('--output_dir', default='/scratch/bepk/bkim2/MultiMAE/result/s1-s2-new/',\n",
    "                    help='Path where to save, empty for no saving')\n",
    "parser.add_argument('--device', default='cuda',\n",
    "                    help='Device to use for training / testing')\n",
    "parser.add_argument('--seed', default=0, type=int, help='Random seed ')\n",
    "parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "parser.add_argument('--auto_resume', action='store_true')\n",
    "parser.add_argument('--no_auto_resume', action='store_false', dest='auto_resume')\n",
    "parser.set_defaults(auto_resume=True)\n",
    "parser.add_argument('--start_epoch', default=0, type=int, metavar='N', help='start epoch')\n",
    "parser.add_argument('--num_workers', default=8, type=int)\n",
    "parser.add_argument('--pin_mem', action='store_true',\n",
    "                    help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem',\n",
    "                    help='')\n",
    "parser.set_defaults(pin_mem=False)\n",
    "parser.add_argument('--find_unused_params', action='store_true')\n",
    "parser.add_argument('--no_find_unused_params', action='store_false', dest='find_unused_params')\n",
    "parser.set_defaults(find_unused_params=True)\n",
    "# Wandb logging\n",
    "parser.add_argument('--log_wandb', default=False, action='store_true',\n",
    "                    help='Log training and validation metrics to wandb')\n",
    "parser.add_argument('--no_log_wandb', action='store_false', dest='log_wandb')\n",
    "parser.set_defaults(log_wandb=False)\n",
    "parser.add_argument('--wandb_project', default='MultiMAE-RGB', type=str,\n",
    "                    help='Project name on wandb')\n",
    "parser.add_argument('--wandb_entity', default='goeulkim', type=str,\n",
    "                    help='User or team name on wandb')\n",
    "parser.add_argument('--wandb_run_name', default='multimae-modis-s2', type=str,\n",
    "                    help='Run name on wandb')\n",
    "parser.add_argument('--show_user_warnings', default=False, action='store_true')\n",
    "# Distributed training parameters\n",
    "parser.add_argument('--world_size', default=1, type=int,\n",
    "                    help='number of distributed processes')\n",
    "parser.add_argument('--local_rank', default=-1, type=int)\n",
    "parser.add_argument('--dist_on_itp', action='store_true')\n",
    "parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.all_domains = args.all_domains.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289c411c-d5fa-42b7-86ed-d599a0c91fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.datasets_chloe import build_multimae_pretraining_dataset\n",
    "\n",
    "# Get dataset\n",
    "dataset = build_multimae_pretraining_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118059\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82641\n",
      "17708\n",
      "17710\n"
     ]
    }
   ],
   "source": [
    "# 2. 비율 정의 (예: 70% train, 15% val, 15% test)\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.15, 0.15\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * train_ratio)\n",
    "n_val = int(n_total * val_ratio)\n",
    "n_test = n_total - n_train - n_val  # 나머지\n",
    "\n",
    "# 3. split\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [n_train, n_val, n_test],\n",
    "    generator=torch.Generator().manual_seed(args.seed)  # reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "# # 2. 비율 정의 (예: 80% train, 20% val, no test)\n",
    "# train_ratio, val_ratio = 0.8, 0.2\n",
    "# n_total = len(dataset)\n",
    "# n_train = int(n_total * train_ratio)\n",
    "# n_val = n_total - n_train \n",
    "\n",
    "# # 3. split\n",
    "# train_dataset, val_dataset = random_split(\n",
    "#     dataset, [n_train, n_val],\n",
    "#     generator=torch.Generator().manual_seed(args.seed)  # reproducibility\n",
    "# )\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144ed908-2b08-45c6-a09d-ba502c1e59db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. DataLoader 생성\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d33b69e-2c1c-467d-88e0-d7e89166a6a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['s1', 's2'])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00402f6d-e8fd-481b-b434-feb8e86c83c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1: torch.Size([32, 2, 224, 224])\n",
      "s2: torch.Size([32, 12, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for key, tensor in batch.items():\n",
    "  print(f\"{key}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8231f04-a115-4b4a-8e93-827a7fb6d43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## From MultiMAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5befd21-bfd5-4c51-8d88-ffd82f78a04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from functools import partial\n",
    "from multimae.criterion import MaskedMSELoss\n",
    "from multimae.input_adapters import PatchedInputAdapter\n",
    "from multimae.output_adapters import SpatialOutputAdapter\n",
    "from utils import create_model, NativeScalerWithGradNormCount as NativeScaler\n",
    "from utils.optim_factory import create_optimizer\n",
    "import utils\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add contrastive loss\n",
    "# def contrastive_loss(z1, z2, temperature=0.1):\n",
    "#     \"\"\"\n",
    "#     Cross-modal contrastive loss (NT-Xent style).\n",
    "    \n",
    "#     Args:\n",
    "#         z1, z2: [B, D] latent vectors from two modalities (e.g., S1 and S2)\n",
    "#         temperature: scaling factor for logits\n",
    "    \n",
    "#     Returns:\n",
    "#         Scalar contrastive loss\n",
    "#     \"\"\"\n",
    "#     # Normalize embeddings\n",
    "#     z1 = F.normalize(z1, dim=1)\n",
    "#     z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "#     # Compute similarity matrix\n",
    "#     logits = torch.matmul(z1, z2.T) / temperature  # [B, B]\n",
    "#     labels = torch.arange(z1.size(0), device=z1.device)\n",
    "\n",
    "#     # Cross entropy loss for both directions\n",
    "#     loss_12 = F.cross_entropy(logits, labels)\n",
    "#     loss_21 = F.cross_entropy(logits.T, labels)\n",
    "\n",
    "#     return (loss_12 + loss_21) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Input Domain Configuration (수정: 채널 수 맞게!)\n",
    "# ------------------------------\n",
    "DOMAIN_CONF = {\n",
    "    's1': {\n",
    "        'channels': 2,\n",
    "        'stride_level': 1,\n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=2),\n",
    "        'output_adapter': partial(SpatialOutputAdapter, num_channels=2),\n",
    "        'loss': MaskedMSELoss, \n",
    "    },\n",
    "    's2': {\n",
    "        'channels': 12,\n",
    "        'stride_level': 1,\n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=12),\n",
    "        'output_adapter': partial(SpatialOutputAdapter, num_channels=12),\n",
    "        'loss': MaskedMSELoss,  \n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Model Builder\n",
    "# ------------------------------\n",
    "def get_model(in_domains, out_domains, patch_size=4, decoder_dim=256):\n",
    "    input_adapters = {\n",
    "        d: DOMAIN_CONF[d]['input_adapter'](stride_level=1, patch_size_full=patch_size)\n",
    "        for d in in_domains\n",
    "    }\n",
    "    output_adapters = {\n",
    "        d: DOMAIN_CONF[d]['output_adapter'](\n",
    "            stride_level=1,\n",
    "            patch_size_full=patch_size,\n",
    "            dim_tokens=decoder_dim,\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            use_task_queries=True,\n",
    "            task=d,\n",
    "            context_tasks=in_domains,\n",
    "            use_xattn=True\n",
    "        )\n",
    "        for d in out_domains\n",
    "    }\n",
    "    return create_model(\n",
    "        \"pretrain_multimae_base\",\n",
    "        input_adapters=input_adapters,\n",
    "        output_adapters=output_adapters,\n",
    "        num_global_tokens=1,\n",
    "        drop_path_rate=0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Training Loop\n",
    "# ------------------------------\n",
    "def train_one_epoch(model, train_loader, tasks_loss_fn, optimizer, device, epoch, loss_scaler, in_domains, out_domains, split=\"train\"):\n",
    "    if split == \"train\":\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    for step, batch in enumerate(metric_logger.log_every(train_loader, 10, header)):\n",
    "        tasks_dict = {t: ten.to(device, non_blocking=True) for t, ten in batch.items()}\n",
    "        input_dict = {t: tasks_dict[t] for t in in_domains if t in tasks_dict}\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds, masks = model(input_dict, num_encoded_tokens=196) # original \n",
    "            # preds, masks, latents = model(input_dict, num_encoded_tokens=196, return_encoded=True) # to add contrastive loss\n",
    "\n",
    "            task_losses = {}\n",
    "            for task in out_domains:\n",
    "                target = tasks_dict[task]\n",
    "                task_losses[task] = tasks_loss_fn[task](preds[task].float(), target)\n",
    "\n",
    "            loss_recon = sum(task_losses.values())\n",
    "\n",
    "            # # Contrastive loss (예: modis vs s2)\n",
    "            # z_s1 = latents['s1'].mean(dim=1)   # [B, D]\n",
    "            # z_s2    = latents['s2'].mean(dim=1)      # [B, D]\n",
    "            # loss_contrast = contrastive_loss(z_s1, z_s2, temperature=0.1)\n",
    "\n",
    "            # loss = loss_recon + 0.1*loss_contrast\n",
    "            loss = loss_recon\n",
    "        \n",
    "        if split == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            grad_norm = loss_scaler(loss, optimizer, parameters=model.parameters(), clip_grad=1.0)\n",
    "            torch.cuda.synchronize()\n",
    "        else:\n",
    "            grad_norm = 0.0  # validation은 grad 없음\n",
    "\n",
    "\n",
    "        metric_logger.update(loss=loss.item(), grad_norm=grad_norm)\n",
    "        for task, l in task_losses.items():\n",
    "            metric_logger.update(**{f'{task}_loss': l.item()})\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            f\"{split}_loss_total\": loss.item(),\n",
    "            f\"{split}_grad_norm\": grad_norm,\n",
    "            **{f\"{split}_{task}_loss\": l.item() for task, l in task_losses.items()}\n",
    "        })\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return metric_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7f5e44-5c57-44c3-875e-70d18de153df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "def main(train_loader, val_loader):\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ----------------- Config -----------------\n",
    "    batch_size = 128\n",
    "    epochs = 1\n",
    "    lr = 5e-5\n",
    "    patch_size = 4\n",
    "    decoder_dim = 256\n",
    "\n",
    "    in_domains = ['s1', 's2']\n",
    "    out_domains = ['s1', 's2']\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"multimae-newdataset\",\n",
    "        name=\"chloe_code\",\n",
    "        entity=\"goeulkim\",\n",
    "        config={\n",
    "            \"epochs\":epochs,\n",
    "            \"lr\":lr,\n",
    "            \"batch_size\":batch_size,\n",
    "            \"patch_size\":patch_size,\n",
    "            \"decoder_dim\":decoder_dim\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ----------------- Model -----------------\n",
    "    model = get_model(in_domains, out_domains, patch_size, decoder_dim).to(device)\n",
    "    \n",
    "    # optimizer = create_optimizer(lr, model)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    loss_scaler = NativeScaler()\n",
    "\n",
    "    tasks_loss_fn = {\n",
    "        d: DOMAIN_CONF[d]['loss'](patch_size=patch_size, stride=1)\n",
    "        for d in out_domains\n",
    "    }\n",
    "\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "    # ----------------- Training -----------------\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch(model, train_loader, tasks_loss_fn, optimizer, device, epoch, loss_scaler, in_domains, out_domains, split=\"train\")\n",
    "        train_one_epoch(model, val_loader, tasks_loss_fn, optimizer, device, epoch, loss_scaler, in_domains, out_domains, split=\"valid\")\n",
    "\n",
    "    # ----------------- Save -----------------\n",
    "    torch.save(model.state_dict(), \"pretrain_multimae.pth\")\n",
    "    wandb.finish()\n",
    "    print(\"✅ Pretrained model saved at pretrain_multimae.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # train_loader already here\n",
    "    # from my_dataset import train_loader\n",
    "    # main(train_loader)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccc2d37-78e3-42e5-abba-b0d8a6e683c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgoeul8604\u001b[0m (\u001b[33mgoeulkim\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/bepk/bkim2/MultiMAE_RGB/MultiMAE/wandb/run-20250918_092433-apfyazig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/goeulkim/multimae-newdataset/runs/apfyazig' target=\"_blank\">chloe_code</a></strong> to <a href='https://wandb.ai/goeulkim/multimae-newdataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/goeulkim/multimae-newdataset' target=\"_blank\">https://wandb.ai/goeulkim/multimae-newdataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/goeulkim/multimae-newdataset/runs/apfyazig' target=\"_blank\">https://wandb.ai/goeulkim/multimae-newdataset/runs/apfyazig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/bkim2/miniforge3/envs/multimae_env/lib/python3.13/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/scratch/bepk/bkim2/MultiMAE_RGB/MultiMAE/utils/native_scaler.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler(enabled=enabled)\n",
      "/u/bkim2/miniforge3/envs/multimae_env/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3834496/4150467846.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/u/bkim2/miniforge3/envs/multimae_env/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63852928-390a-4b50-81d1-6dc0999d0fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## dosnstream for yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c86e89ec-dc32-494e-887e-3b579a2920be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from functools import partial\n",
    "from multimae.criterion import MaskedMSELoss\n",
    "from multimae.input_adapters import PatchedInputAdapter\n",
    "from multimae.output_adapters import SpatialOutputAdapter\n",
    "from utils import create_model, NativeScalerWithGradNormCount as NativeScaler\n",
    "from utils.optim_factory import create_optimizer\n",
    "import utils\n",
    "import wandb\n",
    "\n",
    "# ------------------------------\n",
    "# Domain Configuration (harvest 포함)\n",
    "# ------------------------------\n",
    "DOMAIN_CONF = {\n",
    "    'imagery': {\n",
    "        'channels': 20, \n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=20),\n",
    "        'output_adapter': partial(SpatialOutputAdapter, num_channels=20),\n",
    "        'loss': MaskedMSELoss,\n",
    "    },\n",
    "    'application': {\n",
    "        'channels': 10,  \n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=10),\n",
    "        'output_adapter': partial(SpatialOutputAdapter, num_channels=10),\n",
    "        'loss': MaskedMSELoss,\n",
    "    },\n",
    "    'seeding': {\n",
    "        'channels': 10, \n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=10),\n",
    "        'output_adapter': partial(SpatialOutputAdapter, num_channels=10),\n",
    "        'loss': MaskedMSELoss,\n",
    "    },\n",
    "    'soils': {\n",
    "        'channels': 2,  \n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=2),\n",
    "        'output_adapter': partial(SpatialOutputAdapter, num_channels=2),\n",
    "        'loss': MaskedMSELoss,\n",
    "    },\n",
    "    'harvest': {\n",
    "        'channels': 1,   # target (yield)\n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=1),\n",
    "        'output_adapter': partial(SpatialOutputAdapter, num_channels=1),\n",
    "        'loss': MaskedMSELoss,\n",
    "    },\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Model Builder\n",
    "# ------------------------------\n",
    "def get_model(in_domains, out_domains, patch_size=4, decoder_dim=256):\n",
    "    input_adapters = {\n",
    "        d: DOMAIN_CONF[d]['input_adapter'](stride_level=1, patch_size_full=patch_size)\n",
    "        for d in in_domains\n",
    "    }\n",
    "    output_adapters = {\n",
    "        d: DOMAIN_CONF[d]['output_adapter'](\n",
    "            stride_level=1,\n",
    "            patch_size_full=patch_size,\n",
    "            dim_tokens=decoder_dim,\n",
    "            depth=2,\n",
    "            num_heads=8,\n",
    "            use_task_queries=True,\n",
    "            task=d,\n",
    "            context_tasks=in_domains,\n",
    "            use_xattn=True\n",
    "        )\n",
    "        for d in out_domains\n",
    "    }\n",
    "    return create_model(\n",
    "        \"pretrain_multimae_base\",\n",
    "        input_adapters=input_adapters,\n",
    "        output_adapters=output_adapters,\n",
    "        num_global_tokens=1,\n",
    "        drop_path_rate=0.0\n",
    "    )\n",
    "\n",
    "# ------------------------------\n",
    "# Training Loop\n",
    "# ------------------------------\n",
    "def train_one_epoch(model, loader, tasks_loss_fn, optimizer, device, epoch, loss_scaler, in_domains, out_domains, split=\"train\"):\n",
    "    model.train() if split == \"train\" else model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = f\"{split.capitalize()} Epoch: [{epoch}]\"\n",
    "\n",
    "    for step, batch in enumerate(metric_logger.log_every(loader, 10, header)):\n",
    "        tasks_dict = {t: ten.to(device, non_blocking=True) for t, ten in batch.items()}\n",
    "        input_dict = {t: tasks_dict[t] for t in in_domains if t in tasks_dict}\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds, masks = model(input_dict, num_encoded_tokens=196)\n",
    "            task_losses = {}\n",
    "            for task in out_domains:  # harvest\n",
    "                target = tasks_dict[task]\n",
    "                task_losses[task] = tasks_loss_fn[task](preds[task].float(), target)\n",
    "\n",
    "            loss = sum(task_losses.values())\n",
    "\n",
    "        if split == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            grad_norm = loss_scaler(loss, optimizer, parameters=model.parameters(), clip_grad=1.0)\n",
    "            torch.cuda.synchronize()\n",
    "        else:\n",
    "            grad_norm = 0.0\n",
    "\n",
    "        metric_logger.update(loss=loss.item(), grad_norm=grad_norm)\n",
    "        for task, l in task_losses.items():\n",
    "            metric_logger.update(**{f'{split}_{task}_loss': l.item()})\n",
    "\n",
    "        # ✅ W&B log\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            f\"{split}_loss_total\": loss.item(),\n",
    "            f\"{split}_grad_norm\": grad_norm,\n",
    "            **{f\"{split}_{task}_loss\": l.item() for task, l in task_losses.items()}\n",
    "        })\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "\n",
    "    wandb.log({\n",
    "      \"epoch\": epoch,\n",
    "      f\"{split}_loss_avg\": metric_logger.meters[\"loss\"].global_avg,\n",
    "      **{f\"{split}_{task}_loss_avg\": metric_logger.meters[f\"{split}_{task}_loss\"].global_avg for task in out_domains}\n",
    "\n",
    "    })\n",
    "\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "def main(train_loader, val_loader, test_loader):\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ----------------- Config -----------------\n",
    "    batch_size = 32\n",
    "    epochs = 30\n",
    "    lr = 5e-5\n",
    "    patch_size = 4\n",
    "    decoder_dim = 256\n",
    "\n",
    "    in_domains = ['imagery', 'application', 'seeding', 'soils']\n",
    "    out_domains = ['harvest']   # yield prediction\n",
    "\n",
    "    # ----------------- W&B Init -----------------\n",
    "    wandb.init(\n",
    "        project=\"multimae-JD\",\n",
    "        entity=\"goeulkim\",\n",
    "        name=\"finetune_yield\",\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"patch_size\": patch_size,\n",
    "            \"decoder_dim\": decoder_dim,\n",
    "            \"in_domains\": in_domains,\n",
    "            \"out_domains\": out_domains,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    # ----------------- Model -----------------\n",
    "    model = get_model(in_domains, out_domains, patch_size, decoder_dim).to(device)\n",
    "\n",
    "    # ✅ pretrained weight 로드\n",
    "    state_dict = torch.load(\"pretrain_multimae.pth\", map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)  # harvest head는 새로 초기화됨\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    loss_scaler = NativeScaler()\n",
    "\n",
    "    tasks_loss_fn = {\n",
    "        d: DOMAIN_CONF[d]['loss'](patch_size=patch_size, stride=1)\n",
    "        for d in out_domains\n",
    "    }\n",
    "\n",
    "    # ----------------- Training -----------------\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch(model, train_loader, tasks_loss_fn, optimizer, device, epoch, loss_scaler, in_domains, out_domains, split=\"train\")\n",
    "        train_one_epoch(model, val_loader, tasks_loss_fn, optimizer, device, epoch, loss_scaler, in_domains, out_domains, split=\"val\")\n",
    "\n",
    "    # ----------------- Test -----------------\n",
    "    test_stats = train_one_epoch(model, test_loader, tasks_loss_fn, optimizer, device, epochs, loss_scaler, in_domains, out_domains, split=\"test\")\n",
    "    print(\"✅ Test performance:\", test_stats)\n",
    "\n",
    "    # ----------------- Save -----------------\n",
    "    torch.save(model.state_dict(), \"finetuned_multimae_harvest.pth\")\n",
    "    wandb.finish()\n",
    "    print(\"✅ Fine-tuned model saved at finetuned_multimae_harvest.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from my_dataset import train_loader, val_loader, test_loader\n",
    "    # main(train_loader, val_loader, test_loader)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ab977e-5cb0-4ea3-8e58-1eaae41022a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train_loss_total</td><td>▁</td></tr><tr><td>train_s1_loss</td><td>▁</td></tr><tr><td>train_s2_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>step</td><td>0</td></tr><tr><td>train_grad_norm</td><td>inf</td></tr><tr><td>train_loss_total</td><td>13.19023</td></tr><tr><td>train_s1_loss</td><td>6.73726</td></tr><tr><td>train_s2_loss</td><td>6.45297</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chloe_code</strong> at: <a href='https://wandb.ai/goeulkim/multimae-newdataset/runs/vtkgwxs5' target=\"_blank\">https://wandb.ai/goeulkim/multimae-newdataset/runs/vtkgwxs5</a><br> View project at: <a href='https://wandb.ai/goeulkim/multimae-newdataset' target=\"_blank\">https://wandb.ai/goeulkim/multimae-newdataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250917_211311-vtkgwxs5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/bepk/bkim2/MultiMAE_RGB/MultiMAE/wandb/run-20250917_211740-849cyono</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/goeulkim/multimae-JD/runs/849cyono' target=\"_blank\">finetune_yield</a></strong> to <a href='https://wandb.ai/goeulkim/multimae-JD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/goeulkim/multimae-JD' target=\"_blank\">https://wandb.ai/goeulkim/multimae-JD</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/goeulkim/multimae-JD/runs/849cyono' target=\"_blank\">https://wandb.ai/goeulkim/multimae-JD/runs/849cyono</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/bepk/bkim2/MultiMAE_RGB/MultiMAE/utils/native_scaler.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler(enabled=enabled)\n",
      "/tmp/ipykernel_1439342/2554996541.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(train_loader, val_loader, test_loader)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# ----------------- Training -----------------\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_domains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_domains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     train_one_epoch(model, val_loader, tasks_loss_fn, optimizer, device, epoch, loss_scaler, in_domains, out_domains, split=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ----------------- Test -----------------\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, tasks_loss_fn, optimizer, device, epoch, loss_scaler, in_domains, out_domains, split)\u001b[39m\n\u001b[32m     88\u001b[39m input_dict = {t: tasks_dict[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m in_domains \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tasks_dict}\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast():\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     preds, masks = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_encoded_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m196\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpreds s1:\u001b[39m\u001b[33m\"\u001b[39m, torch.isnan(preds[\u001b[33m'\u001b[39m\u001b[33ms1\u001b[39m\u001b[33m'\u001b[39m]).any().item(), preds[\u001b[33m'\u001b[39m\u001b[33ms1\u001b[39m\u001b[33m'\u001b[39m].abs().max().item())\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpreds s2:\u001b[39m\u001b[33m\"\u001b[39m, torch.isnan(preds[\u001b[33m'\u001b[39m\u001b[33ms2\u001b[39m\u001b[33m'\u001b[39m]).any().item(), preds[\u001b[33m'\u001b[39m\u001b[33ms2\u001b[39m\u001b[33m'\u001b[39m].abs().max().item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/multimae_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/multimae_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/bepk/bkim2/MultiMAE_RGB/MultiMAE/multimae/multimae.py:309\u001b[39m, in \u001b[36mMultiMAE.forward\u001b[39m\u001b[34m(self, x, mask_inputs, task_masks, num_encoded_tokens, alphas, sample_tasks_uniformly, fp32_output_adapters)\u001b[39m\n\u001b[32m    307\u001b[39m     W *= \u001b[38;5;28mself\u001b[39m.input_adapters[\u001b[33m'\u001b[39m\u001b[33msemseg\u001b[39m\u001b[33m'\u001b[39m].stride_level\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     B, C, H, W = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.shape  \u001b[38;5;66;03m# TODO: Deal with case where not all have same shape\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Encode selected inputs to tokens\u001b[39;00m\n\u001b[32m    312\u001b[39m input_task_tokens = {\n\u001b[32m    313\u001b[39m     domain: \u001b[38;5;28mself\u001b[39m.input_adapters[domain](tensor)\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m domain, tensor \u001b[38;5;129;01min\u001b[39;00m x.items()\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m domain \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_adapters\n\u001b[32m    316\u001b[39m }\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "main(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d157dd08-1e2c-4ddb-92e7-6839a26aaf50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MultiMAE_Chloe",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "multimae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
